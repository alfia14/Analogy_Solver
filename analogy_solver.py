# -*- coding: utf-8 -*-
"""A_S.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h_47glCxI1j8M9yee1hHw0Ly3d-SXdtL
"""

"""
The purpose of the program is to solve word analogy problems using word embeddings (GloVe in this case).The program takes 3 input words from the text file and gives the 5 closest words and the cosine distance for each analogy problem. 
The code is taken from the link https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/B%20-%20A%20Closer%20Look%20at%20Word%20Embeddings.ipynb#scrollTo=Ek0h4omALYHH"
The code uses torchtext to use the GloVe model to solve word analogy problems.
"""



import torchtext.vocab

#The name field specifies what the vectors have been trained on, here the 6B means a corpus of 6 billion words.
#The dim argument specifies the dimensionality of the word vectors.
#Loading the the GloVe vectors.
glove = torchtext.vocab.GloVe(name = '6B', dim = 100)  

print(f'There are {len(glove.itos)} words in the vocabulary')

#stoi: string-to-index returns a dictionary of words to indexes
#itos : index-to-string returns an array of words by index
#vectors: returns the actual vectors. 
#get_vector() : a function that takes in embeddings and a word then returns the associated vector. It'll also throw an error if the word doesn't exist in the vocabulary.

def get_vector(embeddings, word):
    assert word in embeddings.stoi, f'*{word}* is not in the vocab!'
    return embeddings.vectors[embeddings.stoi[word]]

# The original code used normal distances, therefore altered it to use cosine similarity.
import torch 

# closest_words: Function to find similar words by taking the input as the word vector, then it scans through the vocabulary calculating the cosine distance between the vector of each word and the input word vector.
# It then sorts these from closest to furthest away.
# The function below returns the closest 5 words to an input word vector:
def closest_words(embeddings, vector, n = 5):
    
    cos = torch.nn.CosineSimilarity(dim=0)
    distances = [(word, cos(vector, get_vector(embeddings, word)).item())
                 for word in embeddings.itos]
    
    return sorted(distances, key = lambda w: w[1], reverse=True)[:n]

#To print the resulting word and the cosine distance
def print_tuples(tuples):
    for w, d in tuples:
        print(f'({d:02.04f}) {w}')

#Function to solve the word analogy problem

def analogy(embeddings, word1, word2, word3, n=5):
    
    #get vectors for each word
    word1_vector = get_vector(embeddings, word1)
    word2_vector = get_vector(embeddings, word2)
    word3_vector = get_vector(embeddings, word3)
    
    #calculate analogy vector
    analogy_vector = word2_vector - word1_vector + word3_vector
    
    #find closest words to analogy vector
    candidate_words = closest_words(embeddings, analogy_vector, n+3)
    
    #filter out words already in analogy
    candidate_words = [(word, dist) for (word, dist) in candidate_words 
                       if word not in [word1, word2, word3]][:n]
    
    print(f'{word1} is to {word2} as {word3} is to...')
    
    return candidate_words

# Reading the inputs from text file and solving analogy for each problem

with open('analogy_problems.txt') as f:
  for i in range(0,10):
    lines = f.readline().split()
    print( )
    print_tuples(analogy(glove, lines[0], lines[1], lines[2]))

"""
 1) Which of your 10 cases showed evidence of bias? Show the results of these analogies and explain the nature of the bias. At least 3 of your cases should illustrate some form of bias.
 -> The evidence of bias was found in following cases:
     1. man muscular woman : The result is skeletal. This shows an evidence of gender bias based on their physical appearance.
     2. she housewife he   : The result is schoolteacher. This is a typical example of gender stereotype. Although, the desired result 'homemaker' is on third rank.
     3. rich doctor poor   : The result is nurse. This is an example of occupational stereotype based on financial status.
     4. he she liberal : The result is conservative. Example of gender-bias.
     5. white lawful black : The result is unlawful. This is an example of racism.
     
 2) What could be done to eliminate this bias from the embeddings? 
 ->  The methods that can be performed to eliminate the bias from the word embeddings are as follows:
    The first step should be:
    Identification of the bias subspace (eg. Gender subspace).One way of doing it is by taking the difference of some pre-known sets that define the concept of bias itself.
    For example- daughter-son, girl-boy for gender subspace. The direction of the bias can be obtained from performing SVD. Then either we can go for Hard Debiasing or Soft Debiasing.
        1. Hard Debiasing: Neutralize and Equalize: The vectors lying in this subspace are “neutralized” such that they are at equal distance from pairs like"boy-girl". 
        Technically, the projection of the embedding on the bias direction is subtracted from the vector.The embeddings outside this direction (the bias-specific terms) are averaged out
        to have the same vector length. This is done to ensure that the neutral terms are equidistant to all equality pairs. For example, the term doctor should be equidistant to both boy-girl and man-woman.
        2. Soft Debiasing: It is possible that sometimes the gender-specific terms contain more meaning that is required. In this case, we can only soften the effect of the bias
        on the embeddings based on certain parameters.
    3.Another way is by removing the bias from the corpus itself in order to eliminate it.One of the technique is 'flipping the raw text' in which the text that possesses bias can be flipped completely to 
    make it unbiased. For example- for the pair 'he-she', 'he is a doctor' can be flipped into 'she is a doctor'.

 (Word Count: 255)
 Sources: 
 'Man is to Computer Programmer as Woman is to Homemaker?Debiasing Word Embeddings' : https://arxiv.org/pdf/1607.06520.pdf  
 'Tackling Gender Bias in Word Embeddings' : https://towardsdatascience.com/tackling-gender-bias-in-word-embeddings-c965f4076a10
 'Attenuating Bias in Word Vectors:http://proceedings.mlr.press/v89/dev19a/dev19a.pdf

3) Should bias be eliminated from embeddings? Why or why not? 

-> It is the fact that the word embeddings are trained on corpuses which have biases and therefore the biases shown by the embeddings simply reflects the existence of biases in the real world. 
And also it is known that the natural language processing should give real world predictions.But according to my understanding, the biases should be removed from the word embeddings in order to
use them for formal purposes. For instance, if someone is planning to apply natural language processing for their business problems, bias is something that one should very strongly consider as 
ignoring them could cause serious ethical consequences. As most of the data sources that are used to train language models, it is possible that they carry a demographic bias.
Therefore, one should always check for the potential implications of bias and ways to mitigate them. Also, it is important to use  the accurate methods for identifying bias and choosing the 
right approach for debiasing. 
Another reason to eliminate the bias can be the fact that word embeddings use the machine learning models that are made to generalise their decisions based on data, thus if there is any bias in
the data, it is likely to get amplified.
(Word Count: 202 ) 
"""

